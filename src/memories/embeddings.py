"""Ollama embedding client with caching, batching, and rate limiting.

Provides :class:`EmbeddingEngine`, the sole interface for producing
and querying vector embeddings in the memories system.  All embeddings
are generated by an Ollama server running a local model (default:
``nomic-embed-text``).

Key features:

* **Caching** -- SHA-256-keyed embedding cache avoids redundant model
  calls for previously-seen text.
* **Batching** -- multiple texts are grouped into batches that respect
  the configured ``ollama_batch_size`` for efficient GPU utilisation.
* **Rate limiting** -- a sliding-window limiter prevents overloading the
  Ollama server beyond ``ollama_max_rps``.
* **Concurrency control** -- an :class:`asyncio.Semaphore` caps the
  number of simultaneous Ollama requests at ``concurrent_embeds``.

Usage::

    from memories.storage import Storage
    from memories.embeddings import EmbeddingEngine

    store = Storage()
    await store.initialize()

    engine = EmbeddingEngine(store)
    vec = await engine.embed_text("Hello world")
    results = await engine.search_similar("find me similar memories", k=5)
"""

from __future__ import annotations

import asyncio
import hashlib
import logging
import time
from collections import deque
from typing import Sequence

import numpy as np
import ollama

from memories.config import get_config
from memories.ollama_manager import OllamaManager
from memories.storage import Storage, deserialize_embedding, serialize_embedding

log = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Sliding-window rate limiter
# ---------------------------------------------------------------------------


class _SlidingWindowRateLimiter:
    """Simple sliding-window rate limiter for async code.

    Tracks timestamps of recent requests within a one-second window and
    sleeps the caller when the window is full.

    Parameters
    ----------
    max_rps:
        Maximum requests per second.
    """

    def __init__(self, max_rps: int) -> None:
        self._max_rps = max_rps
        self._timestamps: deque[float] = deque()

    async def acquire(self) -> None:
        """Wait until a request slot is available.

        Evicts stale timestamps (older than 1 second), then sleeps if
        the window is at capacity before recording the current request.
        """
        now = time.monotonic()

        # Evict timestamps outside the 1-second window.
        while self._timestamps and (now - self._timestamps[0]) >= 1.0:
            self._timestamps.popleft()

        if len(self._timestamps) >= self._max_rps:
            # Sleep until the oldest entry falls out of the window.
            sleep_for = 1.0 - (now - self._timestamps[0])
            if sleep_for > 0:
                await asyncio.sleep(sleep_for)
            # Evict again after sleeping.
            now = time.monotonic()
            while self._timestamps and (now - self._timestamps[0]) >= 1.0:
                self._timestamps.popleft()

        self._timestamps.append(time.monotonic())


# ---------------------------------------------------------------------------
# Embedding engine
# ---------------------------------------------------------------------------


class EmbeddingEngine:
    """Async embedding engine backed by an Ollama server.

    Wraps the Ollama ``embed`` API with a transparent content-hash cache,
    configurable batching, concurrency limiting, and rate limiting.

    Parameters
    ----------
    storage:
        An initialised :class:`~memories.storage.Storage` instance used
        for reading/writing the ``embedding_cache`` and ``atoms_vec``
        tables.
    """

    def __init__(self, storage: Storage) -> None:
        cfg = get_config()

        self._storage = storage
        self._model = cfg.embedding_model
        self._dims = cfg.embedding_dims

        self._client = ollama.AsyncClient(host=cfg.ollama_url)
        self._semaphore = asyncio.Semaphore(cfg.rate_limits.concurrent_embeds)
        self._rate_limiter = _SlidingWindowRateLimiter(cfg.rate_limits.ollama_max_rps)
        self._batch_size = cfg.rate_limits.ollama_batch_size
        self._ollama_url = cfg.ollama_url
        self._ollama_checked = False

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def ensure_ollama_ready(self) -> None:
        """Ensure Ollama is ready before embedding operations.

        Checks that Ollama is installed, the daemon is running, and the
        required embedding model is available. Attempts to start the
        daemon and pull the model if needed.

        This method is idempotent and performs checks only on the first
        call. Subsequent calls return immediately if Ollama was previously
        verified.

        Raises
        ------
        RuntimeError
            If Ollama is not available, the daemon cannot be started, or
            the required model cannot be pulled. The exception message
            contains platform-specific installation or troubleshooting
            instructions.
        """
        if self._ollama_checked:
            return

        manager = OllamaManager(base_url=self._ollama_url, required_model=self._model)
        success, message = await manager.ensure_ready()

        if not success:
            raise RuntimeError(
                f"Ollama is not ready for use:\n\n{message}\n\n"
                f"Required model: {self._model}\n"
                f"Ollama URL: {self._ollama_url}"
            )

        log.info("Ollama ready: %s", message)
        self._ollama_checked = True

    async def embed_text(self, text: str) -> list[float]:
        """Embed a single text string.

        Checks the embedding cache first.  On a cache miss the text is
        sent to the Ollama model and the resulting vector is stored in
        the cache before being returned.

        Parameters
        ----------
        text:
            The text to embed.

        Returns
        -------
        list[float]
            Embedding vector of length ``embedding_dims``.

        Raises
        ------
        RuntimeError
            If the Ollama server is unreachable or returns an error.
        """
        content_hash = self._content_hash(text)

        # Check cache.
        cached = await self._cache_lookup(content_hash)
        if cached is not None:
            return cached

        # Call Ollama (with rate limiting and concurrency control).
        embedding = await self._embed_via_ollama(text)

        # Store in cache.
        await self._cache_store(content_hash, embedding)

        return embedding

    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        """Embed multiple texts, leveraging the cache and batching.

        For each input text the cache is checked first.  Only texts with
        cache misses are sent to Ollama, grouped into batches of
        ``ollama_batch_size``.  All resulting embeddings are stored in
        the cache and the full list is returned in the original order.

        Parameters
        ----------
        texts:
            A list of text strings to embed.

        Returns
        -------
        list[list[float]]
            Embedding vectors in the same order as *texts*.
        """
        if not texts:
            return []

        hashes = [self._content_hash(t) for t in texts]

        # D3: Bulk cache lookup — one SELECT IN instead of N sequential queries.
        placeholders = ",".join("?" * len(hashes))
        cache_rows = await self._storage.execute(
            f"SELECT content_hash, embedding, model FROM embedding_cache "
            f"WHERE content_hash IN ({placeholders})",
            tuple(hashes),
        )
        hit_map: dict[str, list[float]] = {}
        stale_hashes: list[str] = []
        for row in cache_rows:
            if row["model"] != self._model:
                stale_hashes.append(row["content_hash"])
            else:
                hit_map[row["content_hash"]] = deserialize_embedding(row["embedding"])

        if stale_hashes:
            stale_ph = ",".join("?" * len(stale_hashes))
            await self._storage.execute_write(
                f"DELETE FROM embedding_cache WHERE content_hash IN ({stale_ph})",
                tuple(stale_hashes),
            )

        # Pre-populate results from cache.
        results: list[list[float] | None] = [None] * len(texts)
        uncached_indices: list[int] = []

        for idx, content_hash in enumerate(hashes):
            if content_hash in hit_map:
                results[idx] = hit_map[content_hash]
            else:
                uncached_indices.append(idx)

        if not uncached_indices:
            return results  # type: ignore[return-value]

        # Embed uncached texts in batches.
        uncached_texts = [texts[i] for i in uncached_indices]
        all_new_embeddings: list[list[float]] = []

        for batch_start in range(0, len(uncached_texts), self._batch_size):
            batch = uncached_texts[batch_start : batch_start + self._batch_size]
            batch_embeddings = await self._embed_batch_via_ollama(batch)
            all_new_embeddings.extend(batch_embeddings)

        # Place new embeddings into results and batch-write to cache.
        cache_entries: list[tuple] = []
        for offset, idx in enumerate(uncached_indices):
            embedding = all_new_embeddings[offset]
            results[idx] = embedding
            cache_entries.append((hashes[idx], serialize_embedding(embedding), self._model))

        if cache_entries:
            await self._storage.execute_many(
                "INSERT OR REPLACE INTO embedding_cache "
                "(content_hash, embedding, model) VALUES (?, ?, ?)",
                cache_entries,
            )

        return results  # type: ignore[return-value]

    async def embed_and_store(self, atom_id: int, text: str) -> list[float]:
        """Embed text and insert the vector into the ``atoms_vec`` table.

        If an embedding for the same ``atom_id`` already exists it is
        replaced (``INSERT OR REPLACE``).

        Parameters
        ----------
        atom_id:
            The atom row id to associate with the embedding.
        text:
            The text content to embed.

        Returns
        -------
        list[float]
            The embedding vector that was stored.
        """
        embedding = await self.embed_text(text)
        if self._storage.vec_available:
            blob = serialize_embedding(embedding)
            await self._storage.execute_write(
                "INSERT OR REPLACE INTO atoms_vec(atom_id, embedding) VALUES (?, ?)",
                (atom_id, blob),
            )
        return embedding

    async def search_similar(
        self,
        text_or_embedding: str | list[float],
        k: int = 10,
    ) -> list[tuple[int, float]]:
        """Find the *k* most similar atoms by vector distance.

        Uses the sqlite-vec ``MATCH`` query on the ``atoms_vec`` virtual
        table to perform an approximate nearest-neighbour search.

        Parameters
        ----------
        text_or_embedding:
            Either a raw text string (which will be embedded first) or a
            pre-computed embedding vector.
        k:
            Number of results to return.

        Returns
        -------
        list[tuple[int, float]]
            A list of ``(atom_id, distance)`` tuples ordered by ascending
            distance (most similar first).
        """
        if not self._storage.vec_available:
            log.warning("Vector search unavailable -- sqlite-vec not loaded")
            return []

        if isinstance(text_or_embedding, str):
            query_vec = await self.embed_text(text_or_embedding)
        else:
            query_vec = text_or_embedding

        query_blob = serialize_embedding(query_vec)

        rows = await self._storage.execute(
            "SELECT atom_id, distance FROM atoms_vec "
            "WHERE embedding MATCH ? AND k = ?",
            (query_blob, k),
        )
        return [(row["atom_id"], row["distance"]) for row in rows]

    def cosine_similarity(
        self,
        vec_a: list[float],
        vec_b: list[float],
    ) -> float:
        """Compute cosine similarity between two embedding vectors.

        Pure CPU computation with no I/O — intentionally synchronous so that
        callers can invoke it directly without an ``await`` and without
        incurring coroutine scheduling overhead.

        Uses numpy for a vectorised dot-product and norm computation instead
        of a 768-iteration pure-Python loop.

        Parameters
        ----------
        vec_a:
            First embedding vector.
        vec_b:
            Second embedding vector.

        Returns
        -------
        float
            Cosine similarity in the range ``[-1, 1]``.  Returns ``0.0``
            if either vector has zero magnitude.
        """
        if len(vec_a) != len(vec_b):
            raise ValueError(
                f"Vector length mismatch: {len(vec_a)} vs {len(vec_b)}"
            )

        # G1: numpy vectorised computation replaces the 768-element Python loop.
        a = np.asarray(vec_a, dtype=np.float32)
        b = np.asarray(vec_b, dtype=np.float32)
        denom = np.linalg.norm(a) * np.linalg.norm(b)
        return float(np.dot(a, b) / denom) if denom > 0 else 0.0

    async def health_check(self) -> bool:
        """Check whether the Ollama server is reachable and the model exists.

        Sends a minimal embedding request and returns ``True`` if it
        succeeds.  Returns ``False`` (never raises) if the server is
        down or the model is unavailable.
        """
        try:
            async with self._semaphore:
                await self._client.embed(model=self._model, input="health check")
            return True
        except (ConnectionError, OSError, ollama.ResponseError) as exc:
            log.warning("Ollama health check failed: %s", exc)
            return False
        except Exception as exc:
            log.warning("Unexpected error during Ollama health check: %s", exc)
            return False

    # ------------------------------------------------------------------
    # Utility helpers
    # ------------------------------------------------------------------

    def _content_hash(self, text: str) -> str:
        """Return the SHA-256 hex digest of *text* for cache keying."""
        return hashlib.sha256(text.encode("utf-8")).hexdigest()

    @staticmethod
    def distance_to_similarity(distance: float) -> float:
        """Convert an L2 distance from sqlite-vec into a similarity score.

        sqlite-vec ``vec0`` tables use L2 (Euclidean) distance by default.
        This converts to a bounded ``[0, 1]`` similarity value using::

            similarity = max(0, 1 - distance / 2)

        Parameters
        ----------
        distance:
            L2 distance returned by a ``MATCH`` query.

        Returns
        -------
        float
            Similarity in ``[0, 1]`` where 1 means identical.
        """
        return max(0.0, 1.0 - distance / 2.0)

    # ------------------------------------------------------------------
    # Ollama interaction (private)
    # ------------------------------------------------------------------

    async def _embed_via_ollama(self, text: str) -> list[float]:
        """Send a single text to the Ollama ``embed`` endpoint.

        Applies both the concurrency semaphore and the sliding-window
        rate limiter before making the request.

        Raises
        ------
        RuntimeError
            If the Ollama server is unreachable.
        """
        await self._rate_limiter.acquire()

        try:
            async with self._semaphore:
                response = await self._client.embed(
                    model=self._model,
                    input=text,
                )
        except ConnectionError as exc:
            log.error("Ollama server unreachable: %s", exc)
            raise RuntimeError(
                "Cannot generate embedding -- Ollama server is not running. "
                "Please start Ollama and try again."
            ) from exc
        except ollama.ResponseError as exc:
            log.error("Ollama embed error: %s", exc)
            raise RuntimeError(
                f"Ollama embedding request failed: {exc}"
            ) from exc

        embeddings: Sequence[Sequence[float]] = response.embeddings
        if not embeddings:
            raise RuntimeError("Ollama returned an empty embedding response")

        return list(embeddings[0])

    async def _embed_batch_via_ollama(
        self,
        texts: list[str],
    ) -> list[list[float]]:
        """Send a batch of texts to the Ollama ``embed`` endpoint.

        The Ollama ``embed`` API accepts a list of strings as *input*
        and returns a corresponding list of embedding vectors.

        Parameters
        ----------
        texts:
            Batch of texts (should not exceed ``ollama_batch_size``).

        Returns
        -------
        list[list[float]]
            Embeddings in the same order as *texts*.
        """
        if not texts:
            return []

        await self._rate_limiter.acquire()

        try:
            async with self._semaphore:
                response = await self._client.embed(
                    model=self._model,
                    input=texts,
                )
        except ConnectionError as exc:
            log.error("Ollama server unreachable during batch embed: %s", exc)
            raise RuntimeError(
                "Cannot generate embeddings -- Ollama server is not running. "
                "Please start Ollama and try again."
            ) from exc
        except ollama.ResponseError as exc:
            log.error("Ollama batch embed error: %s", exc)
            raise RuntimeError(
                f"Ollama batch embedding request failed: {exc}"
            ) from exc

        embeddings: Sequence[Sequence[float]] = response.embeddings
        if len(embeddings) != len(texts):
            raise RuntimeError(
                f"Ollama returned {len(embeddings)} embeddings for "
                f"{len(texts)} inputs"
            )

        return [list(e) for e in embeddings]

    # ------------------------------------------------------------------
    # Cache operations (private)
    # ------------------------------------------------------------------

    async def _cache_lookup(self, content_hash: str) -> list[float] | None:
        """Look up an embedding in the ``embedding_cache`` table.

        Returns ``None`` on a cache miss or if the cached model does not
        match the currently configured embedding model (stale entry).
        """
        rows = await self._storage.execute(
            "SELECT embedding, model FROM embedding_cache WHERE content_hash = ?",
            (content_hash,),
        )
        if not rows:
            return None

        row = rows[0]
        if row["model"] != self._model:
            # Model changed since the entry was cached -- treat as a miss
            # and delete the stale entry.
            log.debug(
                "Cache entry for %s uses model %s (current: %s); evicting",
                content_hash[:12],
                row["model"],
                self._model,
            )
            await self._storage.execute_write(
                "DELETE FROM embedding_cache WHERE content_hash = ?",
                (content_hash,),
            )
            return None

        return deserialize_embedding(row["embedding"])

    async def _cache_store(
        self,
        content_hash: str,
        embedding: list[float],
    ) -> None:
        """Store an embedding in the ``embedding_cache`` table."""
        blob = serialize_embedding(embedding)
        await self._storage.execute_write(
            "INSERT OR REPLACE INTO embedding_cache "
            "(content_hash, embedding, model) VALUES (?, ?, ?)",
            (content_hash, blob, self._model),
        )
